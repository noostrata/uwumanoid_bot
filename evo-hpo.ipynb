{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEMIFINAL\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import wandb\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "# Define the custom network structure\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes):\n",
    "        super(CustomNet, self).__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_layers[0]), nn.ReLU()]\n",
    "        for i in range(len(hidden_layers) - 1):\n",
    "            layers += [nn.Linear(hidden_layers[i], hidden_layers[i + 1]), nn.ReLU()]\n",
    "        layers.append(nn.Linear(hidden_layers[-1], num_classes))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Training and evaluation logic\n",
    "def train_model(config):\n",
    "    net = CustomNet(config[\"input_size\"], config[\"hidden_layers\"], config[\"num_classes\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        # Add your actual training logic here\n",
    "        inputs = torch.from_numpy(config[\"X_train\"])\n",
    "        labels = torch.from_numpy(config[\"y_train\"])\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, config[\"num_epochs\"], loss.item()))\n",
    "\n",
    "        # Add your actual evaluation logic here\n",
    "        accuracy = evaluate_model(net, config[\"X_test\"], config[\"y_test\"])\n",
    "        tune.report(accuracy=accuracy)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.from_numpy(X_test).float()\n",
    "        y_test_tensor = torch.from_numpy(y_test).long()\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total = y_test_tensor.size(0)\n",
    "        correct = (predicted == y_test_tensor).sum().item()\n",
    "        accuracy = correct / total\n",
    "        return accuracy\n",
    "\n",
    "# Mutation function for PBT\n",
    "def mutate_layers(config):\n",
    "    # Example mutation logic - can be adjusted\n",
    "    new_layers = config[\"hidden_layers\"]\n",
    "    if np.random.rand() < 0.5 and len(new_layers) > 1:\n",
    "        # Remove a layer\n",
    "        new_layers.pop()\n",
    "    else:\n",
    "        # Add a layer\n",
    "        new_layers.append(np.random.choice([32, 64, 128]))\n",
    "    return {\"hidden_layers\": new_layers}\n",
    "\n",
    "# PBT Setup\n",
    "scheduler = PopulationBasedTraining(\n",
    "    time_attr=\"training_iteration\",\n",
    "    perturbation_interval=5,\n",
    "    hyperparam_mutations={\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"hidden_layers\": mutate_layers  \n",
    "    }\n",
    ")\n",
    "\n",
    "# Dummy dataset (replace with your actual dataset)\n",
    "X_train = np.random.rand(1000, 784).astype(np.float32)\n",
    "y_train = np.random.randint(0, 10, 1000).astype(np.long)\n",
    "X_test = np.random.rand(100, 784).astype(np.float32)\n",
    "y_test = np.random.randint(0, 10, 100).astype(np.long)\n",
    "\n",
    "# Run the PBT\n",
    "analysis = tune.run(\n",
    "    train_model,\n",
    "    name=\"pbt_test\",\n",
    "    scheduler=scheduler,\n",
    "    num_samples=4,\n",
    "    config={\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"num_epochs\": 10,\n",
    "        \"input_size\": 784,\n",
    "        \"num_classes\": 2,\n",
    "        \"hidden_layers\": [128],  # Initial layer configuration\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    ")\n",
    "\n",
    "best_config = analysis.get_best_config(metric=\"accuracy\", mode=\"max\")\n",
    "print(\"Best config:\", best_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
