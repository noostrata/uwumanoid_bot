{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray import tune, train\n",
    "from ray import air\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "from ray.air.integrations.wandb import WandbLoggerCallback, setup_wandb\n",
    "from ray.train import Checkpoint\n",
    " \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "# Initialize Ray\n",
    "ray.shutdown()  # Ensure Ray is reset\n",
    "ray.init(ignore_reinit_error=True)\n",
    " \n",
    "def parse_line(line):\n",
    "    features, label = line.strip().split('||')\n",
    "    features = [int(x) for x in features.split(',')]\n",
    "    label = int(label)\n",
    "    return {'features': features, 'label': label}\n",
    " \n",
    "def load_data(file_path, num_lines=100000):\n",
    "    parsed_data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if i >= num_lines:\n",
    "                break\n",
    "            parsed_data.append(parse_line(line))\n",
    "    return ray.data.from_items(parsed_data)\n",
    " \n",
    " \n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes):\n",
    "        super(CustomNet, self).__init__()\n",
    "        self.layers = self._make_layers(input_size, hidden_layers, num_classes)\n",
    "        self.to(device)\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    " \n",
    "    def _make_layers(self, input_size, hidden_layers, num_classes):\n",
    "        layers = [nn.Linear(input_size, hidden_layers[0]), nn.ReLU()]\n",
    "        for i in range(len(hidden_layers) - 1):\n",
    "            layers += [nn.Linear(hidden_layers[i], hidden_layers[i + 1]), nn.ReLU()]\n",
    "        layers.append(nn.Linear(hidden_layers[-1], 1)) \n",
    "        return nn.Sequential(*layers)\n",
    " \n",
    "    def reset_config(self, new_config):\n",
    "        if 'hidden_layers' in new_config:\n",
    "            self.update_architecture(new_config['hidden_layers'])\n",
    "        return True\n",
    " \n",
    "    def update_architecture(self, hidden_layers):\n",
    "        # Extract the number of features for the input and output layers\n",
    "        input_features = self.layers[0].in_features\n",
    "        output_features = self.layers[-1].out_features\n",
    " \n",
    "        # Create a new list of layers\n",
    "        new_layers = [nn.Linear(input_features, hidden_layers[0]), nn.ReLU()]\n",
    " \n",
    "        # Add the new hidden layers\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            new_layers.append(nn.Linear(hidden_layers[i - 1], hidden_layers[i]))\n",
    "            new_layers.append(nn.ReLU())\n",
    " \n",
    "        # Add the output layer\n",
    "        new_layers.append(nn.Linear(hidden_layers[-1], output_features))\n",
    " \n",
    "        # Update the model's layers\n",
    "        self.layers = nn.Sequential(*new_layers)\n",
    " \n",
    "# CHECKPOINT_DIR = r'C:\\Users\\lul\\Desktop\\MLBOT\\uwumanoid_bot\\checkpoints'\n",
    " \n",
    "class CustomTrainable(Trainable):\n",
    "    def setup(self, config):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.config = config\n",
    "        self.model = CustomNet(config[\"input_size\"], config[\"hidden_layers\"], config[\"num_classes\"]).to(self.device)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=config[\"lr\"])\n",
    "        self._iteration = 0 \n",
    " \n",
    "        # Load data here\n",
    "        dataset = load_data(self.config['data_file_path'])\n",
    "        train_dataset, test_dataset = dataset.train_test_split(test_size=0.2)\n",
    " \n",
    "        # Convert to Torch Datasets\n",
    "        self.train_torch_dataset = train_dataset.to_torch(\n",
    "            label_column=\"label\",\n",
    "            feature_columns=[\"features\"],\n",
    "            batch_size=2048,\n",
    "            unsqueeze_label_tensor=False\n",
    "        )\n",
    "        self.test_torch_dataset = test_dataset.to_torch(\n",
    "            label_column=\"label\",\n",
    "            feature_columns=[\"features\"],\n",
    "            batch_size=2048,\n",
    "            unsqueeze_label_tensor=False\n",
    "        )\n",
    " \n",
    " \n",
    "    def step(self):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        num_batches = 0\n",
    " \n",
    "        for features_tensor, labels_tensor in self.train_torch_dataset:\n",
    "            # The tensors are already in the correct format\n",
    "            features_tensor = features_tensor.to(self.device).float()\n",
    "            labels_tensor = labels_tensor.to(self.device).float()\n",
    " \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(features_tensor).squeeze(1)\n",
    "            outputs = outputs.squeeze(1)\n",
    "            loss = self.criterion(outputs, labels_tensor)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    " \n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            self._iteration += 1\n",
    " \n",
    "            if num_batches % 10 == 0:\n",
    "                print(f'Batch {num_batches}, Loss: {loss.item():.4f}')\n",
    " \n",
    " \n",
    "        # Calculate average loss after all batches are processed\n",
    "        epoch_loss = running_loss / num_batches if num_batches > 0 else 0\n",
    " \n",
    "        # Evaluate the model on the test dataset for mean_accuracy\n",
    "        mean_accuracy = evaluate_model(self.model, self.test_torch_dataset, self.device)\n",
    " \n",
    "        # Checkpointing and Reporting\n",
    "        if self._iteration % self.config[\"checkpoint_interval\"] == 0:\n",
    "            checkpoint_path = self.save_checkpoint(self.logdir)\n",
    "            train.report({\"mean_accuracy\": mean_accuracy, \"loss\": epoch_loss, \"checkpoint\": checkpoint_path})\n",
    "        else:\n",
    "            train.report({\"mean_accuracy\": mean_accuracy, \"loss\": epoch_loss})\n",
    " \n",
    "        print(f'Epoch {self._iteration}, Loss: {epoch_loss:.4f}, mean_accuracy: {mean_accuracy:.4f}')\n",
    " \n",
    "        return {\"mean_accuracy\": mean_accuracy, \"loss\": epoch_loss}\n",
    " \n",
    " \n",
    "            # Log metrics to WandB and Ray Tune\n",
    "        # Note: Ensure WandbLoggerCallback is properly set up in your tune.run() call\n",
    "        # wandb.log({\"epoch\": self._iteration, \"loss\": epoch_loss, \"mean_accuracy\": mean_accuracy})\n",
    " \n",
    "    def save_checkpoint(self, checkpoint_dir):\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint.pth\")\n",
    "        torch.save({\n",
    "            \"model_state\": self.model.state_dict(),\n",
    "            \"optimizer_state\": self.optimizer.state_dict()\n",
    "        }, checkpoint_path)\n",
    "        return checkpoint_path\n",
    " \n",
    " \n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    " \n",
    "    def reset_config(self, new_config):\n",
    "        # Update the model's architecture\n",
    "        if 'hidden_layers' in new_config:\n",
    "            self.model.update_architecture(new_config['hidden_layers'])\n",
    " \n",
    "        return True\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "def mutate_layers(config, max_neurons_first_layer=256, min_neurons_last_layer=32):\n",
    "    new_layers = config[\"hidden_layers\"]\n",
    " \n",
    "    mutation_prob = 0.5  # Probability of mutation\n",
    "    layer_add_remove_prob = 0.5  # Probability of adding/removing a layer vs changing a layer size\n",
    " \n",
    "    if np.random.rand() < mutation_prob:\n",
    "        if np.random.rand() < layer_add_remove_prob:\n",
    "            # Adding or removing a layer\n",
    "            if len(new_layers) > 1 and np.random.rand() < 0.5:\n",
    "                # Remove a layer with 50% probability if more than one layer exists\n",
    "                new_layers.pop()\n",
    "            else:\n",
    "                # Add a new layer with a random size (limit the total number of layers if needed)\n",
    "                if len(new_layers) < 5:  # Example limit for total number of layers\n",
    "                    new_layer_size = np.random.choice([32, 64, 128, 256])\n",
    "                    # Ensure the new layer size doesn't exceed the size of the last layer\n",
    "                    if len(new_layers) > 0:\n",
    "                        new_layer_size = min(new_layer_size, new_layers[-1])\n",
    "                    new_layers.append(new_layer_size)\n",
    "        else:\n",
    "            # Changing the size of a random layer\n",
    "            if new_layers:\n",
    "                layer_to_change = np.random.randint(len(new_layers))\n",
    "                new_layer_size = np.random.choice([32, 64, 128, 256])\n",
    "                # Ensure the new layer size respects the size of adjacent layers\n",
    "                if layer_to_change > 0:\n",
    "                    new_layer_size = min(new_layer_size, new_layers[layer_to_change - 1])\n",
    "                if layer_to_change < len(new_layers) - 1:\n",
    "                    new_layer_size = max(new_layer_size, new_layers[layer_to_change + 1])\n",
    "                new_layers[layer_to_change] = new_layer_size\n",
    " \n",
    "    # Enforce constraints on the first and last layers\n",
    "    new_layers[0] = min(new_layers[0], max_neurons_first_layer)\n",
    "    if new_layers:\n",
    "        new_layers[-1] = max(new_layers[-1], min_neurons_last_layer)\n",
    " \n",
    "    return {\"hidden_layers\": new_layers}\n",
    " \n",
    " \n",
    "def evaluate_model(model, test_torch_dataset, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_torch_dataset:\n",
    "            features = features.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    " \n",
    "            # Model predictions\n",
    "            logits = model(features)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "            predictions = (probabilities > 0.5).float()\n",
    " \n",
    "            # Update correct and total count\n",
    "            correct += (predictions.squeeze() == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    " \n",
    "             # # Debugging prints\n",
    "            # print(f\"Features shape: {features.shape}\")\n",
    "            # print(f\"Labels shape: {labels.shape}\")\n",
    "            # print(f\"Logits: {logits}\")\n",
    "            # print(f\"Probabilities: {probabilities}\")\n",
    "            # print(f\"Predictions: {predictions}\")\n",
    "            # print(f\"Correct labels: {labels}\")\n",
    "            # # More debugging prints\n",
    "            # print(f\"Batch correct predictions: {(predictions.squeeze() == labels).sum().item()}\")\n",
    "            # print(f\"Batch total: {labels.size(0)}\")\n",
    "            # print(f\"Cumulative correct predictions: {correct}\")\n",
    "            # print(f\"Cumulative total: {total}\")\n",
    " \n",
    " \n",
    " \n",
    "    mean_accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"Final mean accuracy: {mean_accuracy}\")\n",
    "    return mean_accuracy\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "scheduler = PopulationBasedTraining(\n",
    "    time_attr=\"training_iteration\",\n",
    "    metric=\"mean_accuracy\",\n",
    "    mode=\"max\",\n",
    "    perturbation_interval=5,\n",
    "    hyperparam_mutations={\n",
    "        #\"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"hidden_layers\": mutate_layers  \n",
    "    }\n",
    ")\n",
    " \n",
    "analysis = tune.run(\n",
    "    CustomTrainable,\n",
    "    name=\"pbt_test\",\n",
    "    scheduler=scheduler,\n",
    "    num_samples=1,\n",
    "    reuse_actors=True,\n",
    "    config={\n",
    "        \"lr\": 0.001,\n",
    "        \"checkpoint_interval\": 5,\n",
    "        \"num_epochs\": 10,\n",
    "        \"input_size\": 173,\n",
    "        \"num_classes\": 1,\n",
    "        \"hidden_layers\": [64],\n",
    "        \"data_file_path\": r'C:\\Users\\lul\\Desktop\\MLBOT\\games\\ML1_50m_games.txt',\n",
    "        # Include other configurations here\n",
    "    },\n",
    "   stop={\"training_iteration\": 1000, \"mean_accuracy\": 0.8},\n",
    " \n",
    "    resources_per_trial={\n",
    "        \"cpu\": 1,\n",
    "        \"gpu\": 0.25\n",
    "    },\n",
    "    callbacks=[WandbLoggerCallback(project=\"is-project\", group=\"is-project\", api_key=\"3a2d5a882da8bbcd3651c1ef4a6009a863d89d16\")]\n",
    ")\n",
    " \n",
    " \n",
    "top_trial = analysis.get_best_trial(metric=\"mean_accuracy\", mode=\"max\", scope=\"last-5-avg\")\n",
    "model = CustomNet(top_trial.config[\"input_size\"], top_trial.config[\"hidden_layers\"], top_trial.config[\"num_classes\"]).to(device)\n",
    "checkpoint = analysis.get_best_checkpoint(top_trial, metric=\"mean_accuracy\", mode=\"max\")\n",
    "if checkpoint and os.path.isfile(checkpoint):\n",
    "    model.load_state_dict(torch.load(checkpoint))\n",
    "else:\n",
    "    print(f\"Checkpoint file not found at {checkpoint}\")\n",
    " \n",
    "model_save_directory = r'C:\\Users\\lul\\Desktop\\MLBOT\\uwumanoid_bot\\models'\n",
    "layer_info = \"_\".join(map(str, top_trial.config[\"hidden_layers\"]))\n",
    "model_file_name = f'model_{layer_info}.pth'\n",
    "model_save_path = os.path.join(model_save_directory, model_file_name)\n",
    " \n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Best model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Initialize Ray\u001b[39;00m\n\u001b[1;32m     17\u001b[0m ray\u001b[38;5;241m.\u001b[39mshutdown()  \u001b[38;5;66;03m# Ensure Ray is reset\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_reinit_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Load and parse the data\u001b[39;00m\n\u001b[1;32m     21\u001b[0m total_lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ray/_private/worker.py:1618\u001b[0m, in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, labels, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, storage, **kwargs)\u001b[0m\n\u001b[1;32m   1585\u001b[0m     ray_params \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39m_private\u001b[38;5;241m.\u001b[39mparameter\u001b[38;5;241m.\u001b[39mRayParams(\n\u001b[1;32m   1586\u001b[0m         node_ip_address\u001b[38;5;241m=\u001b[39m_node_ip_address,\n\u001b[1;32m   1587\u001b[0m         object_ref_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1612\u001b[0m         node_name\u001b[38;5;241m=\u001b[39m_node_name,\n\u001b[1;32m   1613\u001b[0m     )\n\u001b[1;32m   1614\u001b[0m     \u001b[38;5;66;03m# Start the Ray processes. We set shutdown_at_exit=False because we\u001b[39;00m\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;66;03m# shutdown the node in the ray.shutdown call that happens in the atexit\u001b[39;00m\n\u001b[1;32m   1616\u001b[0m     \u001b[38;5;66;03m# handler. We still spawn a reaper process in case the atexit handler\u001b[39;00m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;66;03m# isn't called.\u001b[39;00m\n\u001b[0;32m-> 1618\u001b[0m     _global_node \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_private\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshutdown_at_exit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspawn_reaper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mray_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mray_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1625\u001b[0m     \u001b[38;5;66;03m# In this case, we are connecting to an existing cluster.\u001b[39;00m\n\u001b[1;32m   1626\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_cpus \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m num_gpus \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ray/_private/node.py:306\u001b[0m, in \u001b[0;36mNode.__init__\u001b[0;34m(self, ray_params, head, shutdown_at_exit, spawn_reaper, connect_only, default_worker)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# Start processes.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head:\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_head_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m connect_only:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_ray_processes()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ray/_private/node.py:1310\u001b[0m, in \u001b[0;36mNode.start_head_processes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gcs_client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_gcs_server()\n\u001b[0;32m-> 1310\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_gcs_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_cluster_info_to_kv()\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ray_params\u001b[38;5;241m.\u001b[39mno_monitor:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ray/_private/node.py:677\u001b[0m, in \u001b[0;36mNode.get_gcs_client\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_gcs_client\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gcs_client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 677\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_gcs_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gcs_client\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/ray/_private/node.py:693\u001b[0m, in \u001b[0;36mNode._init_gcs_client\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m     gcs_address \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcs_address\n\u001b[0;32m--> 693\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43mGcsClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgcs_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcluster_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ray_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_id \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_cluster_id()\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead:\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;66;03m# Send a simple request to make sure GCS is alive\u001b[39;00m\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;66;03m# if it's a head node.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import ray\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from ray import train, tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "from ray.air.integrations.wandb import WandbLoggerCallback, setup_wandb\n",
    "\n",
    "file_path = 'path_to_your_file/random_random_10k_games.txt' \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Ray\n",
    "ray.shutdown()  # Ensure Ray is reset\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Load and parse the data\n",
    "total_lines = sum(1 for line in open(file_path, 'r'))\n",
    "data_list = []\n",
    "labels_list = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for i, line in enumerate(file, 1):\n",
    "        features, label = line.strip().split('||')\n",
    "        features = [int(x) for x in features.split(',')]\n",
    "        label = int(label)\n",
    "        data_list.append(features)\n",
    "        labels_list.append(label)\n",
    "        \n",
    "        if i % 100000 == 0:\n",
    "            percentage_done = (i / total_lines) * 100\n",
    "            print(f\"Processed {i} lines ({percentage_done:.2f}% completed)\")\n",
    "\n",
    "print(f\"Processed {total_lines} lines (100% completed)\")\n",
    "\n",
    "# Convert lists to NumPy arrays and then to PyTorch tensors\n",
    "data_np = np.array(data_list, dtype=np.float32)\n",
    "labels_np = np.array(labels_list, dtype=np.int64)  \n",
    "data_tensor = torch.from_numpy(data_np)\n",
    "labels_tensor = torch.from_numpy(labels_np)\n",
    "print(\"Data shape:\", data_tensor.shape)\n",
    "\n",
    "# Create a TensorDataset and split it\n",
    "dataset = TensorDataset(data_tensor, labels_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders for the training and testing sets\n",
    "batch_size = 2048\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes):\n",
    "        super(CustomNet, self).__init__()\n",
    "        self.layers = self._make_layers(input_size, hidden_layers, num_classes)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def _make_layers(self, input_size, hidden_layers, num_classes):\n",
    "        layers = [nn.Linear(input_size, hidden_layers[0]), nn.ReLU()]\n",
    "        for i in range(len(hidden_layers) - 1):\n",
    "            layers += [nn.Linear(hidden_layers[i], hidden_layers[i + 1]), nn.ReLU()]\n",
    "        layers.append(nn.Linear(hidden_layers[-1], num_classes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def reset_config(self, new_config):\n",
    "        if 'hidden_layers' in new_config:\n",
    "            self.update_architecture(new_config['hidden_layers'])\n",
    "        return True\n",
    "    \n",
    "    def update_architecture(self, hidden_layers):\n",
    "        # Extract the number of features for the input and output layers\n",
    "        input_features = self.layers[0].in_features\n",
    "        output_features = self.layers[-1].out_features\n",
    "\n",
    "        # Create a new list of layers\n",
    "        new_layers = [nn.Linear(input_features, hidden_layers[0]), nn.ReLU()]\n",
    "\n",
    "        # Add the new hidden layers\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            new_layers.append(nn.Linear(hidden_layers[i - 1], hidden_layers[i]))\n",
    "            new_layers.append(nn.ReLU())\n",
    "\n",
    "        # Add the output layer\n",
    "        new_layers.append(nn.Linear(hidden_layers[-1], output_features))\n",
    "\n",
    "        # Update the model's layers\n",
    "        self.layers = nn.Sequential(*new_layers)\n",
    "\n",
    "\n",
    "def train_model(config, checkpoint_dir=None):\n",
    "    wandb = setup_wandb(config, project=\"is-project\")\n",
    "    model = CustomNet(config[\"input_size\"], config[\"hidden_layers\"], config[\"num_classes\"]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_checkpoint = os.path.join(checkpoint_dir, \"model.pth\")\n",
    "        if os.path.isfile(model_checkpoint):\n",
    "            model.load_state_dict(torch.load(model_checkpoint))\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{config[\"num_epochs\"]}], '\n",
    "                      f'Batch [{batch_idx}/{len(train_loader)}], '\n",
    "                      f'Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Check and apply new configuration at the end of each epoch\n",
    "        new_config = ray.train.get_context().get_trial_dir()  # Get new configuration from PBT\n",
    "        if new_config:\n",
    "            model.reset_config(new_config)  # Apply new configuration to the model\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        accuracy = evaluate_model(model, test_loader)\n",
    "        \n",
    "        wandb.log({\"epoch\": epoch, \"loss\": epoch_loss, \"accuracy\": accuracy})\n",
    "        train.report({\"accuracy\": accuracy, \"loss\": epoch_loss})\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{config[\"num_epochs\"]}], '\n",
    "              f'Loss: {epoch_loss:.4f}, '\n",
    "              f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "        # Save checkpoint at the end of each epoch\n",
    "        with train.checkpoint_dir(step=epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"model.pth\")\n",
    "            torch.save(model.state_dict(), path)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "def mutate_layers(config, max_neurons_first_layer=256, min_neurons_last_layer=32):\n",
    "    new_layers = config[\"hidden_layers\"]\n",
    "    \n",
    "    mutation_prob = 0.5  # Probability of mutation\n",
    "    layer_add_remove_prob = 0.5  # Probability of adding/removing a layer vs changing a layer size\n",
    "\n",
    "    if np.random.rand() < mutation_prob:\n",
    "        if np.random.rand() < layer_add_remove_prob:\n",
    "            # Adding or removing a layer\n",
    "            if len(new_layers) > 1 and np.random.rand() < 0.5:\n",
    "                # Remove a layer with 50% probability if more than one layer exists\n",
    "                new_layers.pop()\n",
    "            else:\n",
    "                # Add a new layer with a random size (limit the total number of layers if needed)\n",
    "                if len(new_layers) < 5:  # Example limit for total number of layers\n",
    "                    new_layer_size = np.random.choice([32, 64, 128, 256])\n",
    "                    # Ensure the new layer size doesn't exceed the size of the last layer\n",
    "                    if len(new_layers) > 0:\n",
    "                        new_layer_size = min(new_layer_size, new_layers[-1])\n",
    "                    new_layers.append(new_layer_size)\n",
    "        else:\n",
    "            # Changing the size of a random layer\n",
    "            if new_layers:\n",
    "                layer_to_change = np.random.randint(len(new_layers))\n",
    "                new_layer_size = np.random.choice([32, 64, 128, 256])\n",
    "                # Ensure the new layer size respects the size of adjacent layers\n",
    "                if layer_to_change > 0:\n",
    "                    new_layer_size = min(new_layer_size, new_layers[layer_to_change - 1])\n",
    "                if layer_to_change < len(new_layers) - 1:\n",
    "                    new_layer_size = max(new_layer_size, new_layers[layer_to_change + 1])\n",
    "                new_layers[layer_to_change] = new_layer_size\n",
    "\n",
    "    # Enforce constraints on the first and last layers\n",
    "    new_layers[0] = min(new_layers[0], max_neurons_first_layer)\n",
    "    if new_layers:\n",
    "        new_layers[-1] = max(new_layers[-1], min_neurons_last_layer)\n",
    "\n",
    "    return {\"hidden_layers\": new_layers}\n",
    "\n",
    "scheduler = PopulationBasedTraining(\n",
    "    time_attr=\"training_iteration\",\n",
    "    metric=\"accuracy\",\n",
    "    mode=\"max\",\n",
    "    perturbation_interval=5,\n",
    "    hyperparam_mutations={\n",
    "        #\"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"hidden_layers\": mutate_layers  \n",
    "    }\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    lambda config: train_model(config),\n",
    "    name=\"pbt_test\",\n",
    "    scheduler=scheduler,\n",
    "    num_samples=4,\n",
    "    config={\n",
    "        \"lr\": 0.001,  # Fixed learning rate\n",
    "        \"num_epochs\": 10,\n",
    "        \"input_size\": 173,  # Updated to match the number of features\n",
    "        \"num_classes\": 2,  # This remains 2 for binary classification (won or lost)\n",
    "        \"hidden_layers\": [64],  # Initial architecture\n",
    "    },\n",
    "    resources_per_trial={\n",
    "            \"cpu\": 1, # 1 CPU core\n",
    "            \"gpu\": 0.25  # 1/4 GPU\n",
    "        },\n",
    "    callbacks=[WandbLoggerCallback(project=\"is-project\", group=\"is-project\", api_key=\"3a2d5a882da8bbcd3651c1ef4a6009a863d89d16\")]\n",
    ")\n",
    "\n",
    "top_trials = analysis.get_best_trials(metric=\"accuracy\", mode=\"max\", limit=5)\n",
    "for idx, trial in enumerate(top_trials, start=1):\n",
    "    model = CustomNet(trial.config[\"input_size\"], trial.config[\"hidden_layers\"], trial.config[\"num_classes\"]).to(device)\n",
    "    checkpoint = analysis.get_best_checkpoint(trial)\n",
    "    model.load_state_dict(torch.load(checkpoint))\n",
    "\n",
    "    # Create a dynamic model name based on architecture\n",
    "    layer_info = \"_\".join(map(str, trial.config[\"hidden_layers\"]))\n",
    "    model_save_path = f'model_{layer_info}_{idx}.pth'\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
